[
["index.html", "Data preparation, processing and interpretation with R Chapter 1 Introduction", " Data preparation, processing and interpretation with R QFAB Chapter 1 Introduction Welcome to QFAB’s Data prepration, processing and reporting with R workshop. This manual contains all the exercises that you will do during this workshop. There will be hints, tips and examples along the way. The format of the workshop will be that we will introduce a topic with some examples, and then set you some practical exercises to carry out. We hope that you will be able to complete these exercises using just the information that you have learnt in the workshop in combination with the R help functions. If you do get stuck though, you will find example answers to all of them at the back of this manual. Please use these examples as a guide and reference rather than just a solution: don’t just copy the code, but think about what each line is doing and see if you can come up with a better way. "],
["how-to-read-this-book.html", "1.1 How to read this book", " 1.1 How to read this book In the shaded dialog boxes with the thick left border are commands that should be typed into your R Console window. This corresponds to input, the output will sometimes follow below with lines preceeded with two hashmarks (##). print(&quot;HELLO WORLD&quot;) ## [1] &quot;HELLO WORLD&quot; Tips, Suggestions and Traps Comments and salient advice are provided in dialogs such as this. We’ll use this format to warn you of traps that you could slip into and to provide some hints. Tips, Suggestions and Traps Warnings are provided in dialogs such as this. We’ll use this format to warn you of traps that you could slip into and to provide some hints. Time for you to make R work Exercises are the best way to learn. A document can provide an insight to the process but hands-on interaction with R is the only way to learn. In the exercise boxes are some suggested exercises that will apply the knowledge that is being shared. "],
["getting-started.html", "1.2 Getting started", " 1.2 Getting started Open an RStudio server client Log onto the server URL that is provided to you by the trainer with the username and password Create a New Project that you will use for this course using the File &gt; New Project in RStudio. To keep things tidy, use the option to create this project in a new directory (e.g. “Data interpretation”) # Move into the new working directory WORKSHOP_DIR &lt;- file.path(&quot;~&quot;,&quot;Data_Workshop&quot;) dir.create(WORKSHOP_DIR) setwd(WORKSHOP_DIR) dir.create(&#39;figure&#39;) file.copy(&quot;../data/Data_Interpretation/logo.png&quot;,&quot;figure/logo.png&quot;) "],
["a-quick-refresher.html", "1.3 A quick refresher", " 1.3 A quick refresher If you haven’t used R for a while, or are a relatively new R user, then take a moment to remind yourself about the various data types in R. Try creating an example of the different types of structure using various data types: Numeric Integer Logical Character Complex And the structures in which those data types can be stored: Vector Matrix List Data frame Factors (not really a structure!) -->"],
["data-preparation.html", "Chapter 2 Data Preparation", " Chapter 2 Data Preparation Before we can process and analyze data files in R to generate a report, we first need to collect, collate, and prepare the data for R. How you set up and define your data is critical to the future value of that data. Implementing a good data management strategy need not be an onerous task; it may simply require the adoption of a number of standard practices by members of a research group, and can have many benefits, including: Better usability and so increased value of data within a research group, along with reduced training time for new staff and less risk of loss of key knowledge when staff leave Easier error checking and correction, as well as avoidance of misinterpretation caused by ambiguous or unclear data Lower time and financial cost of analysis: well-formatted data is simpler and easier to import into analysis software, and also simplifies engagement with core analysis facilities and external service providers If data meets with community standards, it is easier to submit it to public resources to support research publications. Researchers who share their data have been shown to be cited more often In addition to these benefits, funding bodies such as the ARC now require a data management plan and researchers and their data may be subject to regulatory and institutional obligations, such as the Code for the Responsible Conduct of Research. "],
["data-lifecycle-management-plan.html", "2.1 Data Lifecycle &amp; Management Plan", " 2.1 Data Lifecycle &amp; Management Plan A Data Management Plan is a document that describes how you will conduct the Data Lifecycle for your project. This would include: Collecting data design research plan data management (formats, storage etc) plan consent for sharing locate existing data collect data (experiment, observe, measure, simulate) capture and create metadata Processing data enter data, digitise, transcribe, translate check, validate, clean data anonymise data where necessary describe data manage and store data Preserving data migrate data to best format migrate data to suitable medium back-up and store data create metadata and documentation archive data Giving Access to data distribute data share data control Access establish copyright promote data Re-using data follow-up research new research undertake research reviews scrutinise findings teach and learn "],
["good-data-protocol.html", "2.2 Good Data Protocol", " 2.2 Good Data Protocol In preparing your data for analysis in R you will want to consider using the following approaches: Data Log Archiving or Version control Data Dictionary (Meta Data) Data Monitoring and Safety Data Cleaning and Preprocessing Data Formatting Summary Graphs &amp; Stats Today’s workshop will focus on the last two steps: Data Formatting and Summary Graphs. For more information on the remainder, the two supporting documents can be found at the following links: QFAB Good Data Guidelines QFAB Data Specs "],
["further-reading.html", "2.3 Further Reading", " 2.3 Further Reading Key Data Concepts Wickham, H.. (2014) Tidy Data. JStatSoft Hulley S. B. et al. (2013) Designing Clinical Research. Ch16-17 McFadden E.. (2007) Management of Data in Clinical Trials -->"],
["reproducible-reporting-in-r.html", "Chapter 3 Reproducible reporting in R ", " Chapter 3 Reproducible reporting in R "],
["literate-programming.html", "3.1 Literate programming", " 3.1 Literate programming Literate programming is a computing approach where the programming code is provided as subsections in a document that is mostly written in a natural language. This creates an environment where the documentation is the main component of the program, rather than an afterthought. The text is typically written in some sort of ’mark-up’ format, with sections highlighted as title, emphasised or italicised and so on. The code itself can be run on demand in combination with this marked-up text, to produce a report or other document combining the pre-defined text in the literate program with the output of the code. In terms of using R literate programming techniques for bioinformatics, what this allows us to do is to generate a reusable report template that can be repeatedly run as each lot of data becomes available. The output may be a pdf file suitable for printing or e-mailing, html code for direct display through a web browser, or a number of other formats. For this workshop, we will use the knitR format, which is built directly into the RStudio platform and generates an html report at the click of a button. "],
["open-a-new-knitr-document.html", "3.2 Open a new knitR document", " 3.2 Open a new knitR document Under the RStudio File menu, select New File &gt; R Markdown Give it an appropriate title and introduction If you are not already familiar with the knitR Markdown syntax, click on the question mark button and select Markdown Quick Reference Once you have made some changes, save it using the disk icon, then use the knit HTML button to generate and view your HTML report Optional try some of the Markdown syntax in shown in the Markdown Quick Reference For the rest of this workshop, try to incorporate all the exercises that we do, along with comments and notes, into this or other knitR documents (you may want to create new documents for different datasets). The goal is that at the end of the workshop, you will be able to produce a final, reproducible report of all the analyses that you have done. -->"],
["importing-and-reformatting-data-files.html", "Chapter 4 Importing and reformatting data files ", " Chapter 4 Importing and reformatting data files "],
["importing-data.html", "4.1 Importing data", " 4.1 Importing data R provides a variety of tools for reading in data from a file. One of the more useful ones for bioinformatics data is the read.table function, which we will use to read in the first test dataset. This file contains microarray gene expression results from the widely-used Golub et al. (1999) paper. This data set contains calls from around 7,000 gene probes on samples from 38 leukemia patients. Twenty seven patients are diagnosed as having acute lymphoblastic leukemia (\\(ALL\\)) and eleven as having acute myeloid leukemia (\\(AML\\)). Read in the datafile The data file is in the folder /data/Data_Interpretation and is called data_set_ALL_AML_train.tsv. Note: Remember the location of the data file uses a relative path to find the dataset, so it depends on the current directory from which you are running the Markdown document. If you are running the document from the newly created Data_Workshop folder, then you need to use the relative path ../data/Data_Interpretation/data_set_ALL_AML_train.tsv to read the file. This will go up one directory using the .. annotation for parent directory. Use the head() and readLines() functions to look at the structure of the file With the read.table() function, read the file contents into a data frame Use the second column (labelled Gene Accession Number, actually the microarray probe ID) as the row names "],
["reformat-the-file.html", "4.2 Reformat the file", " 4.2 Reformat the file We now have a data frame containing the following: Column number Contents Row names Microarray probe ID 1 Gene names/descriptions 2, 4, 6… (up to 54) Intensity calls for 27 ALL patients 56, 58, 60… (up to 76) Intensity calls for 11 AML patients 3, 5, 7… (up to 77) Status calls for the intensity values To make this data easier to work with, we want to transform this data frame into a matrix containing just the intensity calls. We will also need to create an object to store the gene names associated with the probe identifiers; this could be a vector of gene names with probe IDs as item names, a two column matrix (probe ID, gene name), or even a list, whichever you prefer. Use a matrix slice to create a matrix of just the intensity values from the golub data frame Hints: use all the even numbered columns As an example of a slice, matrix.name[, c(2,4,6)] will output all rows for columns 2, 4 and 6 of the matrix matrix.name Create a vector, matrix or other object to link gene names with probe identifiers We should now have two objects: (i) the numerical data in one matrix and (ii) the gene name information in whatever format we chose to use. The next stage is to transform and normalise the data "],
["transform-data.html", "4.3 Transform data", " 4.3 Transform data Our data is now in a structured format suitable for further filtering and processing. The first step that we will carry out will be to apply a floor and ceiling to the values in the dataset. Microarrays, like many other measurement platforms in biology, may give inaccurate readings at the high and low end of their operating range. In this case, any values of less than 100 or more than 16,000 are likely to be unreliable. Also, because expression values are typically spread over an extreme range (in this case, around 2.5 orders of magnitude), it is conventional to log transform them to convert the data into a more linear and simple to analyse form. Often in transformation steps, it is good practice to create a new data object to store the transformed values. Remember to appropriately name the data objects so that you remember it has been transformed, e.g. data.norm to indicate the data has been normalised. If the data being analysed requires lots of memory, then remove the raw data after it has been transformed to free up some space. Replace all values &lt;100 with the value 100 Hint: a command of the format matrix.name[matrix.name &lt; 100] will provide the positions of all the matrix elements with a value of less than 100 Replace all values &gt;16,000 with the value 16,000 Convert all values to their log10 equivalent "],
["normalise-data.html", "4.4 Normalise data", " 4.4 Normalise data There is one final stage to go with this data set before it is suitable for full analysis. To allow us to make valid comparisons between the data from different arrays, they need to be normalised. To do this, use the scale() function to give all samples (i.e. columns in the matrix) a mean value of zero and a standard deviation of one. Compare the means and standard deviations of the samples using the apply() function Normalise the data in the intensity values matrix using scale() Use the apply() function again to confirm the normalised means and standard deviations Hint: review the function of the apply() and scale() commands using the R help function "],
["export-data.html", "4.5 Export data", " 4.5 Export data To conclude this section, export the data out of R as a comma separated file (csv). This can them be opened in other data processing applications, or reloaded back into R at a later stage for further analysis. At this time, it would also be a very good idea to save your markup document (which you should have been doing regularly anyway), and save the R workspace so that next time we need to use this data, you can load all the objects from file rather than having to rebuild them again. Use the write.table() function to export the data matrix to a csv file Save the markup document (and knit the HTML report if you have not done so recently) Save the workspace -->"],
["data-manipulation-extracting-and-exporting-records.html", "Chapter 5 Data manipulation - extracting and exporting records ", " Chapter 5 Data manipulation - extracting and exporting records "],
["removing-samples.html", "5.1 Removing samples", " 5.1 Removing samples Our statistical colleagues have now performed some tests on the Golub data, and have warned us that there are some outliers in the dataset. These are characterised by low or high mean intensity values, and large or small standard deviations. Being statisticians who like a joke though, they haven’t told us which samples are affected. To be careful, we will remove the eight most extreme samples, two with the highest and two with the lowest mean intensity, and likewise two each with the highest and lowest standard deviation Some functions that you might find useful in this exercise are apply, colMeans, order, which.min and which.max. Using the full Golub matrix (before log transforming and normalisation), identify and remove: The two samples with the highest mean The two samples with the lowest mean The two samples with the highest standard deviation The two samples with the lowest standard deviation Warning: When performing such data transformations, becareful about how you are performing the steps. Do you perform each filtration one after the other? That is, filter (a) then filter (b) then filter (c) and then filter (d). or Do you first find the answers to (a), (b), (c) and (d) before performing the filtration in one go? Try both approaches above and compare your results. Hint: After each step (a) to (d) look at which sample has the two highest or lowest mean and standard deviation. "],
["remove-uninformative-measurements.html", "5.2 Remove uninformative measurements", " 5.2 Remove uninformative measurements The Golub dataset that we have been using so far contains around 7,000 measurements (gene intensities). Much of this data may be redundant or uninformative, so in this section we will explore ways of filtering out low-value information from a dataset. During our transformation stage, we applied a floor and ceiling value to our measurements, because values outside of this range are considered unreliable. If all the measurements for a gene were below the floor value, or above the ceiling, we can conclude that the expression value of that gene is uniformly outside the useful range of measurement, and ignore the data for that gene. Also, since this is a differential expression analysis, we are not interested in genes which show low levels of variability across the samples, so we will remove them too. Use the apply() function to generate a new matrix which excludes all genes with an intensity of 100 or less across all samples (use the dataset from which you have removed the outliers). Hint: break the problem down first, find the maximum intensity for each gene, then identify for which genes that maximum is more than 100, then finally slice just those rows to a new matrix Further filter this new matrix to remove all genes with an intensity of 16,000 or more for all samples Finally, filter the matrix once more to remove all genes with less than a five-fold change in signal across the samples "],
["extracting-specific-measurements.html", "5.3 Extracting specific measurements", " 5.3 Extracting specific measurements To make up for their practical joke earlier, our statistician friends have now come back and provided us with a list of the one hundred genes that their analysis tells them are most important in differentiating between the \\(ALL\\) and \\(AML\\) groups in our experiment. They’ve also admitted that there weren’t really any outliers after all. To study these further, we therefore want to retrieve the log transformed and normalised intensity values of just these one hundred probes for all 38 of our original samples. Read in the list of 100 probe IDs and associated gene names from the data file “golub100.txt” Hint: the gene names are in column 2 of this tab-delimited file, the probe IDs in column 1 Generate a new matrix of log normalised values for all 38 samples and the 100 genes in this list We will now try further practice on an alternative input dataset. Before doing this, it would again be worth saving your R workspace and your markdown code. Perhaps open a new markdown document for this second dataset -->"],
["integrating-datasets.html", "Chapter 6 Integrating datasets", " Chapter 6 Integrating datasets So far in this workshop, we have concentrated on reading in and analysing the contents of a single file. In this section, we are going to look at what we can do if the data is in multiple files that need to be combined or linked. "],
["combining-two-files-with-identical-row-identifiers.html", "6.1 Combining two files with identical row identifiers", " 6.1 Combining two files with identical row identifiers The first situation might be where the output from our measurement platform is produced in one file for each sample, and then needs to be merged so all results for all samples are in the same matrix or dataframe for analysis. In the simplest case here, the order and length of the files is identical, so they can just be joined as columns in the same dataset. You can do this using the cbind() function # Create several vectors of equal length. Each represents the data from one sample vec.one &lt;- (1:10) vec.two &lt;- (2:11) vec.three &lt;- c(rep(1,3), rep(4,3), rep(8,4)) # Then join them together to make a matrix with one column per sample vector joined.data &lt;- cbind(vec.one, vec.two, vec.three) joined.data ## vec.one vec.two vec.three ## [1,] 1 2 1 ## [2,] 2 3 1 ## [3,] 3 4 1 ## [4,] 4 5 4 ## [5,] 5 6 4 ## [6,] 6 7 4 ## [7,] 7 8 8 ## [8,] 8 9 8 ## [9,] 9 10 8 ## [10,] 10 11 8 Depending on the needs of your analysis, you can alternatively use the rbind() function to join them as rows. cbind will work to join matrices as well as vectors, as long as they all have the same number of rows. Try this using some pre-prepared subsets of the Golub data. Load in the three datasets stored in file golub_cbind.RData using the load() function. This file contains three objects: golub.names - a vector of the gene names golub.all - a dataframe of the ALL sample data, and golub.aml - a dataframe of the AML sample data Use cbind() to create a duplicate of the Golub dataframe from these three objects. 6.1.1 Combining two files with rows in common cbind relies on the datasets being of the same size and in the same order. But what if that is not the case, and not all values are recorded for all samples? Well, if there is a common identifier between the datasets, we can use that to merge two dataframes. As an example, we will try with the built-in R datasets beaver1 and beaver2, which give body temperature plots of two beavers over a period of about 18 hours. For a full list and descriptions of R’s built in example datasets, use the data() command head(beaver1) ## day time temp activ ## 1 346 840 36.33 0 ## 2 346 850 36.34 0 ## 3 346 900 36.35 0 ## 4 346 910 36.42 0 ## 5 346 920 36.55 0 ## 6 346 930 36.69 0 # For this example, we will merge the tables on the &#39;time&#39; column, so we can # start to look how beavers&#39; body temperature fluctuates through the day dim(beaver1) ## [1] 114 4 dim(beaver2) ## [1] 100 4 beaver.temp &lt;- merge(beaver1, beaver2, by = &quot;time&quot;, all = TRUE, sort = FALSE) dim(beaver.temp) ## [1] 115 7 head(beaver.temp) ## time day.x temp.x activ.x day.y temp.y activ.y ## 1 930 346 36.69 0 307 36.58 0 ## 2 940 346 36.71 0 307 36.73 0 ## 3 950 346 36.75 0 307 36.93 0 ## 4 1000 346 36.81 0 307 37.15 0 ## 5 1010 346 36.88 0 307 37.23 0 ## 6 1020 346 36.89 0 307 37.24 0 What’s happening here? Function/Argument Description by is the column to match the two datasets with. You can use column names, numbers or (to merge on rownames) row.names. Note, if you sort by row.names, then an extra column will be created in the output object containing the rownames information, while the rownames themselves of this new object will revert to 1, 2, 3… all = TRUE means that a row will be created for every time point in either of the two samples. If one sample is missing data at that point, then values of “NA” will be entered in the output sort = FALSE by default, merge sorts the output object in the order of the merge column. In this dataset, the timecourse starts in mid-morning, and continues until after midnight the next day, so sorting is not appropriate in this case. Unfortunately, even turning off sort doesn’t do what we want in this case. Rather than leaving the rows in the original order, they are reordered but in a somewhat unpredicatable way. There are functions in non-core R packages (like join in the plyr package that address this, but for the moment, a quick and dirty way is to add 2400 to all times after midnight and then sort the data by the time column: beaver.temp[beaver.temp[, &#39;time&#39;] &lt; 800, &#39;time&#39;] &lt;- beaver.temp[beaver.temp[, &#39;time&#39;] &lt; 800, &#39;time&#39;] + 2400 beaver.temp &lt;- beaver.temp[order(beaver.temp[,&#39;time&#39;]),] The golub_cbind.RData file you loaded earlier contains two, partially overlapping, datasets from the ALL and AML samples (all.subset and aml.subset). Use merge() to join these two dataframes using rownames as identifiers For the dataframe generated by the merge, relabel the rownames correctly, and remove the now redundant “Row.names” column from the dataframe -->"],
["charts-and-graphical-output.html", "Chapter 7 Charts and graphical output", " Chapter 7 Charts and graphical output R has a wide range of built in charts and graphics, and with packages such as ggplot, even more advanced plotting functions are available. In many cases, a crude but valid plot can be produced simply by supplying a matrix or vector directly to one of the plot functions. "],
["standard-r-plotting-functions.html", "7.1 Standard R plotting functions", " 7.1 Standard R plotting functions 7.1.1 Plot() function plot() can be used for several other applications. These include line graphs (showing, for example, changes over time) and volcano plots (for gene expression analysis). Let’s revisit this using our beaver temperature dataset, and preparing a scatterplot of our time-merged data. Make a scatterplot comparing body temperature of the two beavers from our merged dataframe Review the plot and plot.default documentation to add a title to the plot By default, the axis names are probably not what we want. See if you can replace them with Beaver 1 and Beaver 2. Try changing the plot symbols, and maybe adding a trend line. This gives us a correlation, but does not show how data changes over time. For this we need a different type of plot, either type = &quot;l&quot; or type = &quot;o&quot;. The former gives a line plot, the latter an ’overplot’ (points connected by lines). Drawing this plot is a little more involved, since we need to plot the two datasets separately, both scaled against time. # Before we start anything we need to know the maximum range required for # the axes. By default, when we plot the graph, it will scale for the first # animal, but adding others may go outside the display range temp.range &lt;- range(beaver.temp$temp.x, beaver.temp$temp.y, na.rm = TRUE) # Set up the plot for the first animal # In this case, we&#39;ll define the axis labels at the outset # xaxt = &#39;n&#39; means not to give the x-axis scale - we&#39;ll add that in a moment plot(beaver.temp$temp.x, type=&quot;o&quot;, col=&quot;blue&quot;, ylim = temp.range, xaxt = &#39;n&#39;, xlab = &quot;Time&quot;, ylab=&quot;Temp&quot;) # Now add the second animal data lines(beaver.temp$temp.y, type = &quot;o&quot;, col=&quot;red&quot;) #Put some time-points on the x-axis. This command puts 19 time points, on the hour axis(1, at = 3+6*c(0:19), lab = beaver.temp$time[3+6*c(0:19)]) # Finally, give it a title title(main=&quot;Diurnal temperature variation&quot;) Further timecourse plotting practice Using another built-in R dataset Indometh , plot time course changes for two or more of the six samples Give each sample a different colour Try to label and scale the axes appropriately 7.1.2 Boxplots means &lt;- colMeans(golub.log) boxplot(golub.log[,1:15],las=2,main=&quot;Log10 transformed microarray expression&quot;) points(1:15, means[1:15], pch=15, col=&#39;red&#39;) means &lt;- colMeans(golub.norm) boxplot(golub.norm[,1:15],las=2,main=&quot;Normalised microarray expression&quot;) points(1:15, means[1:15], pch=15, col=&#39;red&#39;) 7.1.3 Heatmaps Another commonly used way of displaying biological datasets is the heatmap. This is commonly used for gene expression analysis, but it can be used for any dataset where multiple quantitative measurements are carried out on several samples. Not only can an R-generated heatmap help in the visualisation of these sort of datasets, but the heatmap command can also be used to automatically organise samples and/or measurements into dendrograms, showing the similarity relationship between them. For a first example, let’s plot a heatmap for our 100 key Golub dataset genes. # Run the simplest heatmap, with all default settings heatmap(golub.100) # This works reasonably well, but conventianally expression heatmaps # are in red/green scale # We can load a pre-defined colour palette from the &#39;gplots&#39; package library(gplots) heatmap(golub.100, col = redgreen(100)) By default, this actually gives a very presentable plot. Both rows and columns are automatically sorted to cluster most alike genes and samples together, and dendrograms are plotted for us automatically. Encouragingly, we can see that all the \\(AML\\) and \\(ALL\\) samples form two separate clusters. It might look better without the crowded gene names column, but that can be turned off easily with the setting labRow = NA. For more practice, see if you can prepare a heat plot of the Indometh dataset: Hints: you will need to convert the dataset into a matrix with one column per Subject. Set column names as Subject number and rownames as timepoints review the Rowv setting to keep time points in order in the dendrogram think about transforming the data to compensate for the exponential decrease of the concentration value apply a more attractive colour scheme ( ?redgreen ) 7.1.4 PCA plots The final plotting option we will explore is principle component analysis, or PCA. PCA is often used where there are two or more categories of data, and multiple quantitative measurements on each, to quickly review whether the difference between the categories outweighs any noise or batch effect signals. Essentially, PCA collapses the dataset down to a small number of representative dimensions which can be plotted on a chart. PCA analysis is widely used in expression datasets, which have large numbers of measurements (one per gene), multiple replicates, and normally two or more categorical conditions, so we will test it with our Golub dataset # We will use the PCA function of the &#39;mixOmics&#39; package library(mixOmics) # Carry out the PCA on just the top 100 genes, with default parameters # N.B. PCA data needs to have the samples in rows. Our data is by # columns, so we need to transpose it golub.100.pca &lt;- pca(t(golub.100)) # Then plot the data for the first two components, colouring the # sample names for clarity plotIndiv(golub.100.pca, comp = c(1,2), col = c(rep(&#39;red&#39;, 27), rep(&#39;blue&#39;, 11))) Optional challenge exercises Generate a PCA plot for other components of the analysis Carry out a PCA analysis for the full (normalised, transformed) Golub dataset and generate a plot Does the data differentiate between the two sample types? "],
["ggplot2.html", "7.2 ggplot2", " 7.2 ggplot2 Most of the plots that we have generated in this workshop have been using built-in R plotting functions. These are generally adequate, but for the highest quality charts we may choose to use a specialist package such as ggplot2. This is an extremely powerful and adaptable graphics package, but at the expense of being more challenging to learn and use than the standard tools. Today we will just explore ggplot2 briefly to draw some better line graphs than earlier, but there are plenty of books and web resources to help you learn more, http://docs.ggplot2.org/current/ is a great place to start. One of the first challenges of ggplot is that it doesn’t take data in the ’wide’ format of the matrices and dataframes we have been using up to now - i.e. where a row contains measurements from multiple samples, or multiple measurements from one sample. Instead, data has to be reformatted into ’long’ format, with just one measurement per row (along with sample identifiers, categorical variables and so on). To get our data into the right format, we will use another tool, melt from the reshape2 package. # Start by loading reshape2 library(reshape2) # We&#39;ll work again with the beaver temperature dataset. Before we do the melt # command, it would be sensible to give the dataframe more meaningful column # names than the default &#39;temp.x&#39; and &#39;temp.y&#39; applied when we created it earlier # Set &#39;temp.x&#39; to be &#39;Beaver 1&#39; and &#39;temp.y&#39; to be &#39;Beaver 2&#39; colnames(beaver.temp)[which(colnames(beaver.temp) == &#39;temp.x&#39;)] &lt;- &quot;Beaver 1&quot; colnames(beaver.temp)[which(colnames(beaver.temp) == &#39;temp.y&#39;)] &lt;- &quot;Beaver 2&quot; # Now change the data to a long format beaver.melt &lt;- melt(beaver.temp, measure.vars = c(&quot;Beaver 1&quot;, &quot;Beaver 2&quot;), variable.name = &quot;Animal&quot;, value.name = &quot;Temperature&quot;) # Review the format of this new table head(beaver.melt) ## time day.x activ.x day.y activ.y Animal Temperature ## 1 840 346 0 NA NA Beaver 1 36.33 ## 2 850 346 0 NA NA Beaver 1 36.34 ## 3 900 346 0 NA NA Beaver 1 36.35 ## 4 910 346 0 NA NA Beaver 1 36.42 ## 5 920 346 0 NA NA Beaver 1 36.55 ## 6 930 346 0 307 0 Beaver 1 36.69 # The plot command starts with our &#39;melted&#39; data set, and specifies our x and y data # columns, and the column indicating how we&#39;re going to group and colour the lines ggplot(beaver.melt, aes(factor(time), Temperature, group = Animal, col = Animal)) + geom_line() + # Then specify that it&#39;s a line chart labs(x = &quot;Time&quot;, title = &quot;Diurnal temperature variation&quot;) + # Add some labels theme(plot.title=element_text(size=24,face=&quot;bold&quot;)) + # Format title style scale_x_discrete(breaks = beaver.temp$time[3+6*c(0:19)]) # Label x-axis with one point per hour This graph still needs a little more work, particularly to restore the x-axis back to proper time stamps rather than continuing past 2400, but you can already see that it is a nicer looking plot than we got earlier with the standard R line plot. Once again using the Indometh dataset, plot the same chart as before using ggplot. Hint: The Indometh data is already in long format, so you will not need the melt() command. Even more exercises If you are ahead, have a look at the built-in iris dataset. Try the following standard plots: boxplot(iris[,1:4], main=“Disrbution of Iris properties”) plot(iris\\(Sepal.Length, iris\\)Sepal.Width, pch=16, col=iris$Species, main=“Sepal length vs width”, xlab=“Sepal Length”, ylab=“Sepal Width”) Now reshape the dataset into the long format. iris.long &lt;- melt(iris, measure.vars = colnames(iris)[1:4]) head(iris.long) Or in your own R studio install the tidyr package which is easier to use. library(tidyr) iris.long &lt;- gather(iris,feature,value,-Species) head(iris.long) Experiment with other ggplot functions: ggplot(iris.long) + geom_boxplot(aes(x=feature,y=value)) ggplot(iris.long) + geom_violin(aes(x=feature,y=value)) ggplot(iris.long) + geom_boxplot(aes(x=feature,y=value,col=Species)) ggplot(iris.long) + geom_boxplot(aes(x=feature,y=value,col=Species)) + facet_grid(.~Species) ggplot(long.form) + geom_boxplot(aes(x=feature,y=value,col=Species)) + facet_grid(feature~Species) Use the ggplot2 documentation http://docs.ggplot2.org/current/ to find out what does the function facet_grid() do. -->"],
["generating-report-with-r.html", "Chapter 8 Generating report with R", " Chapter 8 Generating report with R Hopefully through this workshop, you have been editing your R code in a knitR (R markdown) document, and now have a record of all the analyses that you have carried out over the course of the workshop. The final exercise is to tidy up these documents to produce the final report. Typically at QFAB, we might produce two R markdown documents for an analysis. The first is a template which contains all the R code, commentry on what the code is doing, section headers and so on, while the second is build around this template with data-specific interpretive comments inserted where relevant. The first document allows us to reuse the template and re-run the same analysis pipeline on a comparable input sample at a later date, while the second forms the final analysis report for a specific sample (the comments have to be inserted after the analysis of course, because the same analysis process on two different samples may well give two very different outcomes). Working with one (or more) of your knitR documents, tidy up the code and documentation to produce a reusable template format of that dataset. Create a copy of the template and add some interpretation and commentary to generate a final, reproducible report Hints: Use the RStudio File &gt; Save As function to create a copy of the template for the report Give the chunks names to make their purpose easier to remember (to do this, use the format {r chunk_name}, instead of just {r} at the start of the chunk) You can hide R code in your report but still include the output from that code by adding echo = FALSE to the chunk header (e.g. {r chunk_name, echo = FALSE} ). If you want to run R code without showing either the code or the output, use include = FALSE See http://kbroman.org/knitr knutshell/pages/Rmarkdown.html for descriptions of some of the other chunk display options Once you have generated your final knitR report, click on the Save As link at the top of the preview window, and you can download the report to your local computer in HTML format. When using the web-based version of RStudio, this is the only option to save knitR output; however, if you install and run R and RStudio locally, you can also generate PDF and Word format reports. "],
["other-reporting-formats.html", "8.1 Other reporting formats", " 8.1 Other reporting formats During this workshop, we have focussed on using knitR. You may have noticed that under the New File menu option, there are a number of other formats. These all provide similar functionality to knitR, but with various different strengths and weaknesses. This workbook is written in Sweave, which is somewhat better at producing high-quality PDF format documents, but is also less user friendly than knitR and cannot generate HTML output. The best way to learn about these other formats is just to install R and RStudio on your own computer and start experimenting! "],
["interactive-reporting-with-shiny.html", "8.2 Interactive reporting with Shiny", " 8.2 Interactive reporting with Shiny Using knitR, Sweave or any other version of literate programming can give us reproducible, automated data analysis and report generation. However, the reports produced in this way are ultimately still static, with graphs and charts locked to the specifications of the R code. Shiny is an R tool that can be used to overcome this, by generating dynamic HTML plots that can be changed by the viewer to suit their preferences. As an example, a static report may define a histogram with data divided into five levels. Using Shiny, a viewer of this data may choose to look in higher resolution, with 10 or 12 levels; conversely, they may generate a very broad summary of the data in just three levels. Similarly, another analysis may output a list of samples that are relevant at a p-value of 0.05, but with Shiny, the user can change that selection to list samples significant only at 0.01, or 0.001. Unfortunately Shiny does not work well with the web-based RStudio version that we are using for this workshop, so in this session your instructor will demonstrate examples of interactive reporting with Shiny. These are based on the examples at http://shiny.rstudio.com/tutorial/, so if you have R and RStudio installed on your laptop, you are welcome to try them out for yourself at the same time. Even if you don’t join in now, have a go at them later - the examples on this page are specifically designed to help you teach yourself how to use Shiny. -->"],
["conclusion.html", "Chapter 9 Conclusion", " Chapter 9 Conclusion That brings us to the end of the workshop. Don’t worry if you didn’t manage to complete all the exercises - we didn’t expect that you would be able to. Instead, try to take some time over the next few days or weeks to return to the workbook and spend a little more time practicing. You can continue to access your account on the training server for the next few weeks, or you can install R and RStudio on your own machine and download the example files from the training resource site. And don’t forget, before you go, save your R workspace and download this and all your markdown documents to your laptop. That way, you will be able to recreate everything you have done today quickly and simply when you come back to it. "],
["appendix-exercise-answers.html", "Appendix: Exercise answers ", " Appendix: Exercise answers "],
["importing-and-reformatting-data-files-1.html", "4 Importing and reformatting data files", " 4 Importing and reformatting data files 4.1 Read in the datafile ## [1] &quot;Gene Description\\tGene Accession Number\\t1\\tcall\\t2\\tcall\\t3\\tcall\\t4\\tcall\\t5\\tcall\\t6\\tcall\\t7\\tcall\\t8\\tcall\\t9\\tcall\\t10\\tcall\\t11\\tcall\\t12\\tcall\\t13\\tcall\\t14\\tcall\\t15\\tcall\\t16\\tcall\\t17\\tcall\\t18\\tcall\\t19\\tcall\\t20\\tcall\\t21\\tcall\\t22\\tcall\\t23\\tcall\\t24\\tcall\\t25\\tcall\\t26\\tcall\\t27\\tcall\\t34\\tcall\\t35\\tcall\\t36\\tcall\\t37\\tcall\\t38\\tcall\\t28\\tcall\\t29\\tcall\\t30\\tcall\\t31\\tcall\\t32\\tcall\\t33\\tcall&quot; ## [2] &quot;AFFX-BioB-5_at (endogenous control)\\tAFFX-BioB-5_at\\t-214\\tA\\t-139\\tA\\t-76\\tA\\t-135\\tA\\t-106\\tA\\t-138\\tA\\t-72\\tA\\t-413\\tA\\t5\\tA\\t-88\\tA\\t-165\\tA\\t-67\\tA\\t-92\\tA\\t-113\\tA\\t-107\\tA\\t-117\\tA\\t-476\\tA\\t-81\\tA\\t-44\\tA\\t17\\tA\\t-144\\tA\\t-247\\tA\\t-74\\tA\\t-120\\tA\\t-81\\tA\\t-112\\tA\\t-273\\tA\\t-20\\tA\\t7\\tA\\t-213\\tA\\t-25\\tA\\t-72\\tA\\t-4\\tA\\t15\\tA\\t-318\\tA\\t-32\\tA\\t-124\\tA\\t-135\\tA&quot; ## [3] &quot;AFFX-BioB-M_at (endogenous control)\\tAFFX-BioB-M_at\\t-153\\tA\\t-73\\tA\\t-49\\tA\\t-114\\tA\\t-125\\tA\\t-85\\tA\\t-144\\tA\\t-260\\tA\\t-127\\tA\\t-105\\tA\\t-155\\tA\\t-93\\tA\\t-119\\tA\\t-147\\tA\\t-72\\tA\\t-219\\tA\\t-213\\tA\\t-150\\tA\\t-51\\tA\\t-229\\tA\\t-199\\tA\\t-90\\tA\\t-321\\tA\\t-263\\tA\\t-150\\tA\\t-233\\tA\\t-327\\tA\\t-207\\tA\\t-100\\tA\\t-252\\tA\\t-20\\tA\\t-139\\tA\\t-116\\tA\\t-114\\tA\\t-192\\tA\\t-49\\tA\\t-79\\tA\\t-186\\tA&quot; ## [4] &quot;AFFX-BioB-3_at (endogenous control)\\tAFFX-BioB-3_at\\t-58\\tA\\t-1\\tA\\t-307\\tA\\t265\\tA\\t-76\\tA\\t215\\tA\\t238\\tA\\t7\\tA\\t106\\tA\\t42\\tA\\t-71\\tA\\t84\\tA\\t-31\\tA\\t-118\\tA\\t-126\\tA\\t-50\\tA\\t-18\\tA\\t-119\\tA\\t100\\tA\\t79\\tA\\t-157\\tA\\t-168\\tA\\t-11\\tA\\t-114\\tA\\t-85\\tA\\t-78\\tA\\t-76\\tA\\t-50\\tA\\t-57\\tA\\t136\\tA\\t124\\tA\\t-1\\tA\\t-125\\tA\\t2\\tA\\t-95\\tA\\t49\\tA\\t-37\\tA\\t-70\\tA&quot; ## [5] &quot;AFFX-BioC-5_at (endogenous control)\\tAFFX-BioC-5_at\\t88\\tA\\t283\\tA\\t309\\tA\\t12\\tA\\t168\\tA\\t71\\tA\\t55\\tA\\t-2\\tA\\t268\\tA\\t219\\tM\\t82\\tA\\t25\\tA\\t173\\tA\\t243\\tM\\t149\\tA\\t257\\tA\\t301\\tA\\t78\\tA\\t207\\tA\\t218\\tA\\t132\\tA\\t-24\\tA\\t-36\\tA\\t255\\tA\\t316\\tA\\t54\\tA\\t81\\tA\\t101\\tA\\t132\\tA\\t318\\tA\\t325\\tA\\t392\\tP\\t241\\tA\\t193\\tA\\t312\\tA\\t230\\tP\\t330\\tA\\t337\\tA&quot; ## [6] &quot;AFFX-BioC-3_at (endogenous control)\\tAFFX-BioC-3_at\\t-295\\tA\\t-264\\tA\\t-376\\tA\\t-419\\tA\\t-230\\tA\\t-272\\tA\\t-399\\tA\\t-541\\tA\\t-210\\tA\\t-178\\tA\\t-163\\tA\\t-179\\tA\\t-233\\tA\\t-127\\tA\\t-205\\tA\\t-218\\tA\\t-403\\tA\\t-152\\tA\\t-146\\tA\\t-262\\tA\\t-151\\tA\\t-308\\tA\\t-317\\tA\\t-342\\tA\\t-418\\tA\\t-244\\tA\\t-439\\tA\\t-369\\tA\\t-377\\tA\\t-209\\tA\\t-396\\tA\\t-324\\tA\\t-191\\tA\\t-51\\tA\\t-139\\tA\\t-367\\tA\\t-188\\tA\\t-407\\tA&quot; # Let&#39;s set up a few shortcuts first DATADIR &lt;- &quot;../data/Data_Interpretation&quot; Golub.file &lt;- file.path(DATADIR, &quot;data_set_ALL_AML_train.tsv&quot;) # Inspect the file content head(readLines(Golub.file)) # Read the file into the golub.df dataframe golub.df &lt;- read.table(Golub.file,sep=&quot;\\t&quot;, quote=&quot;&quot;, header=T, row.names=2, comment.char=&quot;&quot;, stringsAsFactors = FALSE) 4.2 Reformat the file # Create a golub matrix containing just the intensity values # 2*[1:38] returns even numbers from 2 to 2x38 (i.e. 76) golub.matrix &lt;- as.matrix(golub.df[,2*(1:38)]) # Name the columns with the sample status # The first 27 are &quot;ALL&quot;, the final 11 are &quot;AML&quot; colnames(golub.matrix) &lt;- c(rep(&quot;ALL&quot;,27),rep(&quot;AML&quot;,11)) # Then create a vector of gene names golub.gnames &lt;- golub.df$Gene.Description # or golub.gnames &lt;- golub.df[,1] names(golub.gnames) &lt;- rownames(golub.df) 4.3 Transform data # Find the positions of values in golub below our floor or above our ceiling. # Then replace the values in those positions by our floor or ceiling value. golub.matrix[golub.matrix &lt; 100] &lt;- 100 golub.matrix[golub.matrix &gt; 16000] &lt;- 16000 # Log_10 transformation golub.log &lt;- log(golub.matrix,10) 4.4 Normalise data # Check the existing mean and standard deviation apply(golub.log, 2, mean) ## ALL ALL ALL ALL ALL ALL ALL ALL ## 2.412221 2.393360 2.441669 2.391413 2.390291 2.341846 2.384387 2.399676 ## ALL ALL ALL ALL ALL ALL ALL ALL ## 2.444495 2.338539 2.363399 2.274308 2.386571 2.388623 2.395650 2.380295 ## ALL ALL ALL ALL ALL ALL ALL ALL ## 2.482811 2.324345 2.328069 2.499497 2.306725 2.313102 2.331595 2.409902 ## ALL ALL ALL AML AML AML AML AML ## 2.343512 2.352611 2.360633 2.382545 2.336199 2.427405 2.401558 2.386542 ## AML AML AML AML AML AML ## 2.406045 2.296222 2.400678 2.361889 2.418098 2.451635 apply(golub.log, 2, sd) ## ALL ALL ALL ALL ALL ALL ALL ## 0.5071723 0.5187630 0.5263826 0.4988002 0.5032199 0.4792356 0.4942427 ## ALL ALL ALL ALL ALL ALL ALL ## 0.5020369 0.5447880 0.4846996 0.4989016 0.4477448 0.5113511 0.5011462 ## ALL ALL ALL ALL ALL ALL ALL ## 0.5190887 0.4920936 0.5537143 0.4649320 0.4727689 0.5794321 0.4687063 ## ALL ALL ALL ALL ALL ALL AML ## 0.4640572 0.4649017 0.5043298 0.4843511 0.4877606 0.4773038 0.5001143 ## AML AML AML AML AML AML AML ## 0.4653518 0.5418554 0.5233421 0.5008276 0.5119954 0.4655509 0.5325124 ## AML AML AML ## 0.5000293 0.5186136 0.5380754 # Make average log value for each column equal to zero, then divide all # values by the standard deviation golub.norm &lt;- scale(golub.log, center = TRUE, scale = TRUE) # Now recheck means and standard deviations apply(golub.norm, 2, mean) ## ALL ALL ALL ALL ALL ## -1.986263e-17 -5.360228e-17 2.814013e-16 9.856386e-17 -4.490310e-16 ## ALL ALL ALL ALL ALL ## -3.114529e-16 -1.577204e-16 3.022949e-16 3.314441e-16 2.359088e-16 ## ALL ALL ALL ALL ALL ## -3.865295e-16 5.235524e-17 3.505657e-16 1.426972e-16 1.317842e-16 ## ALL ALL ALL ALL ALL ## -1.581130e-16 3.095132e-16 -9.346855e-17 4.140777e-16 -3.024501e-16 ## ALL ALL ALL ALL ALL ## 2.738962e-16 -9.476412e-17 4.037920e-16 6.162612e-17 -4.122564e-16 ## ALL ALL AML AML AML ## -3.695595e-16 -2.939020e-16 6.263294e-17 3.219093e-16 2.935077e-16 ## AML AML AML AML AML ## -3.169440e-16 -2.172312e-16 -1.518818e-16 2.158345e-16 3.435190e-16 ## AML AML AML ## 2.039186e-17 -1.589449e-16 -7.778114e-17 apply(golub.norm, 2, sd) ## ALL ALL ALL ALL ALL ALL ALL ALL ALL ALL ALL ALL ALL ALL ALL ALL ALL ALL ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## ALL ALL ALL ALL ALL ALL ALL ALL ALL AML AML AML AML AML AML AML AML AML ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## AML AML ## 1 1 4.5 Export data # Export a csv version of the matrix write.table(golub.norm, file=&quot;golub_normalised_data.csv&quot;,sep=&quot;,&quot;, quote=FALSE) # OR write.csv(golub.norm, file=&quot;golub_normalised_data.csv&quot;) # And of the gene name conversion table write.table(golub.gnames, file=&quot;golub_gene_names.csv&quot;, sep=&quot;,&quot;, quote=TRUE) Save the knitR document by just clicking on the disk icon at the top of the knitR markdown document. This may be greyed out if you have just created HTML, since knitR saves the file automatically before running. Save your workspace by going to the Session menu at the top of the RStudio window, and selecting “Save Workspace As” "],
["data-manipulation.html", "5.1 Data manipulation", " 5.1 Data manipulation 5.1.2 Removing samples # See what we&#39;re starting with dim(golub.matrix) ## [1] 7129 38 # Identify and remove the two samples with the highest and lowest mean # Yes, we can do this in one, rather complicated fcommand golub.no.outliers &lt;- golub.matrix[,-(order(colMeans(golub.matrix))[ c(1,2,ncol(golub.matrix),ncol(golub.matrix)-1)])] # Leaving us with just 34 columns dim(golub.no.outliers) ## [1] 7129 34 What’s happening here? Function Description colMeans is calculating the mean for each column (you could also use apply) order is giving the column numbers in ascending order of mean value c(1,2, ncol...) retrieves the first two and the last two entries from that order list - i.e. the column numbers with the two lowest and highest means Finally we take a slice of the golub matrix, exluding those four columns using the - annotation. # Repeat the process for standard deviation. There is no colSD function, # so we will use apply instead golub.no.outliers &lt;- golub.no.outliers[,-(order(apply(golub.no.outliers, 2,sd))[ c(1,2,ncol(golub.no.outliers),ncol(golub.no.outliers)-1)])] While there is a certain satisfaction in building a command that identifies and removes the two highest and lowest mean samples in a single step, any solution which reliably produces the same outcome is a suitable answer. There are any number of ways to answer this exercise. For example, you could create a vector containing the mean values for each column and use the which.min and which.max commands on that to find which column numbers to remove. 5.1.2 Remove uninformative measurements # See what we&#39;re starting with dim(golub.no.outliers) ## [1] 7129 30 # Remove all rows with a maximum value of 100 golub.filtered &lt;- golub.no.outliers[which(apply(golub.no.outliers, 1, max) &gt; 100),] # Or, broken down into stages # i) Identify the maximum value in each row tmp.max &lt;- apply(golub.no.outliers, 1, max) # ii) Identify for which rows the maximum value is &gt; 100 tmp.max.filter &lt;- which(tmp.max &gt; 100) # iii) Take a slice of the matrix keeping only those rows with a max &gt; 100 golub.filtered &lt;- golub.no.outliers[tmp.max.filter,] # Similarly, remove all rows with a minimum of 16,000 # Remember to use the matrix that has already been filtered for low values # in the step above as input golub.filtered &lt;- golub.filtered[which(apply(golub.filtered, 1, min) &lt; 16000),] # Finally, remove rows with less than five fold change across samples golub.filtered &lt;- golub.filtered[which(apply(golub.filtered, 1, max)/ (apply(golub.filtered, 1, min)) &gt; 5),] # Now check the size of the final matrix dim(golub.filtered) ## [1] 2902 30 5.1.3 Extracting specific measurements # First, read in the file and convert the contents into a named vector golub100.gnames.tmp &lt;- read.table(file.path(DATADIR, &quot;golub100.txt&quot;), stringsAsFactors = FALSE) golub100.gnames &lt;- golub100.gnames.tmp[,2] names(golub100.gnames) &lt;- golub100.gnames.tmp[,1] # From the log normalised matrix, extract the 100 rows of interest golub.100 &lt;- golub.norm[names(golub100.gnames),] "],
["integrating-and-summarising-datasets.html", "6 Integrating and summarising datasets", " 6 Integrating and summarising datasets "],
["combining-two-files-with-identical-row-identifiers-1.html", "Combining two files with identical row identifiers", " Combining two files with identical row identifiers # Load the R data file load(file.path(DATADIR, &quot;golub_cbind.RData&quot;)) # Check the objects to make sure that they have the same number of elements/rows # Remember, golub.names is a vector, the others are dataframes length(golub.names) ## [1] 7129 dim(golub.all) ## [1] 7129 55 dim(golub.aml) ## [1] 7129 21 # Then just join them together golub.new.df &lt;- cbind(golub.names, golub.all, golub.aml) dim(golub.new.df) ## [1] 7129 77 "],
["combining-two-files-with-rows-in-common-1.html", "Combining two files with rows in common", " Combining two files with rows in common # Merge the two datasets by &#39;row.names&#39; # Set all = TRUE to include values present in only one or other set (or # experiment with other options) # Turn off sorting, although for expression data, that&#39;s probably unnecessary golub.merge &lt;- merge(all.subset, aml.subset, by = &#39;row.names&#39;, all = TRUE, sort = FALSE) # Reassign rownames rownames(golub.merge) = golub.merge[,1] # And finally, remove the &#39;Row.names&#39; column golub.merge &lt;- golub.merge[,-1] head(golub.merge[,1:5]) ## X1 call X2 call.1 X3 ## AFFX-BioB-M_st -41 A 19 A 19 ## AFFX-BioB-3_st -831 A -743 A -1135 ## AFFX-BioC-5_st -653 A -239 A -962 ## AFFX-BioC-3_st -462 A -83 A -232 ## AFFX-BioDn-5_st 75 A 182 A 208 ## AFFX-BioDn-3_st 381 A 164 A 432 # And if we want to add back in gene names golub.merge.named &lt;- merge(golub.names, golub.merge, by = &#39;row.names&#39;, all.y = TRUE) rownames(golub.merge.named) = golub.merge.named[,1] golub.merge.named &lt;- golub.merge.named[,-1] # Have a quick look at what we&#39;ve ended up with golub.merge.named[1:3,1:6] ## x X1 call X2 call.1 ## A28102_at GB DEF = GABAa receptor alpha-3 subunit 151 A 263 P ## AB000114_at Osteomodulin 72 A 21 A ## AB000115_at mRNA 281 A 250 P ## X3 ## A28102_at 88 ## AB000114_at -27 ## AB000115_at 358 "],
["charts-and-graphical-output-1.html", "7 Charts and graphical output", " 7 Charts and graphical output Plot # Make a scatterplot beaver.temp &lt;- merge(beaver1, beaver2, by = &quot;time&quot;, all = TRUE, sort = FALSE) plot(beaver.temp$temp.x, beaver.temp$temp.y) # Add a title to the plot title(main=&quot;Beaver temperature correlation plot&quot;) # Change the axis labels title(xlab = &#39;Beaver 1&#39;) Oops, this doesn’t work - it just overwrites the existing axis label # Redraw the initial graph without the default labels plot(beaver.temp$temp.x, beaver.temp$temp.y,ann = FALSE) # Now we can add the title and axis labels title(main = &quot;Beaver temperature correlation plot&quot;, xlab = &quot;Beaver 1&quot;, ylab = &quot;Beaver 2&quot;) We can change point types to a ’x’ symbol, but we have to draw the graph yet again. plot(beaver.temp$temp.x, beaver.temp$temp.y,ann = FALSE, pch = 4) title(main = &quot;Beaver temperature correlation plot&quot;, xlab = &quot;Beaver 1&quot;, ylab = &quot;Beaver 2&quot;) # And add a trend line lm.beaver &lt;- lm(beaver.temp[c(&#39;temp.y&#39;,&#39;temp.x&#39;)]) abline(lm.beaver, col = &#39;blue&#39;) Heatmaps # Set up the matrix for our data # This takes concentration data from Indometh 11 rows at a time indo.matrix = matrix(Indometh$conc, nrow = 11) # Then apply Subject number as column names and timepoints as rownames colnames(indo.matrix) = 1:6 rownames(indo.matrix) &lt;- Indometh$time[1:11] # Then plot the heatmap. # Rowv = NA keeps timepoints in order # Scale = &#39;none&#39; stops R trying to rescale colours heatmap(indo.matrix, Rowv = NA, scale = &#39;none&#39;) # But it still has a few problems. The time points are last to first, the # colours are ugly, and everything from 2h on is the same colour heatmap(log(indo.matrix[11:1, ]),scale=c(&quot;none&quot;), Rowv = NA, col = redblue(75)) # That&#39;s better - reverse the row order of the matrix supplied, log # transform the data, and use a red-blue scaling PCA plots # Carry out the PCA analysis. Don&#39;t forget to transpose the dataset golub.full.pca &lt;- pca(t(golub.norm)) # Then plot the results plotIndiv(golub.full.pca, comp = c(1,2),col = c(rep(&#39;red&#39;, 27), rep(&#39;blue&#39;, 11))) Looking at the outcome from both this plot and the selected one hundred genes, there is a clear separation between the ALL and the AML samples. This is a good indication that there is a real biological difference between the two categories which has been captured in the expression data "],
["ggplot2-1.html", "6.2 ggplot2", " 6.2 ggplot2 Draw the Indometh graph using ggplot2: # First, activate the ggplot2 library library(ggplot2) ggplot(Indometh, aes(factor(time), conc, col = Subject, group = Subject)) + geom_line() + labs(x = &quot;Time (h)&quot;, y = &quot;Concentration (mcg/ml)&quot;, title = &quot;Indometh Pharmacokinetics&quot;) + theme(plot.title=element_text(size=20,face=&quot;bold&quot;)) What’s happening here? Function/Argument Description aes sets the aesthetics of the plot. In this case, that we want to plot ’time’ on the x-axis (the factor), ’conc’ on the y-axis, and group and colour the lines by ’Subject’ goem_line() specifies that this is a line plot labs() sets the x and y axis labels and the plot title theme() in this case defines the plot title as bold 24 point + plut signs at the end of the lines tells R that the command hasn’t finished and that it continues on the next line -->"],
["appendix-sessioninfo-for-this-version-of-the-course-material.html", "Appendix - SessionInfo for this version of the course material", " Appendix - SessionInfo for this version of the course material ## R version 3.3.0 (2016-05-03) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 10240) ## ## locale: ## [1] LC_COLLATE=English_Australia.1252 LC_CTYPE=English_Australia.1252 ## [3] LC_MONETARY=English_Australia.1252 LC_NUMERIC=C ## [5] LC_TIME=English_Australia.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] reshape2_1.4.2 mixOmics_6.1.1 ggplot2_2.2.0 lattice_0.20-34 ## [5] MASS_7.3-45 gplots_3.0.1 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_0.12.8 RColorBrewer_1.1-2 plyr_1.8.4 ## [4] bitops_1.0-6 tools_3.3.0 digest_0.6.10 ## [7] jsonlite_1.2 evaluate_0.10 tibble_1.2 ## [10] gtable_0.2.0 igraph_1.0.1 DBI_0.5-1 ## [13] shiny_0.14.2 rstudioapi_0.6 yaml_2.1.14 ## [16] parallel_3.3.0 dplyr_0.5.0 stringr_1.1.0 ## [19] knitr_1.15.1 htmlwidgets_0.8 gtools_3.5.0 ## [22] caTools_1.17.1 rprojroot_1.1 grid_3.3.0 ## [25] ellipse_0.3-8 R6_2.2.0 rgl_0.96.0 ## [28] rmarkdown_1.2 bookdown_0.3 gdata_2.17.0 ## [31] tidyr_0.6.0 corpcor_1.6.8 magrittr_1.5 ## [34] backports_1.0.4 scales_0.4.1 htmltools_0.3.5 ## [37] assertthat_0.1 xtable_1.8-2 mime_0.5 ## [40] colorspace_1.3-1 httpuv_1.3.3 labeling_0.3 ## [43] KernSmooth_2.23-15 stringi_1.1.2 lazyeval_0.2.0 ## [46] munsell_0.4.3 "]
]
